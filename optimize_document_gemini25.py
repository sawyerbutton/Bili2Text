#!/usr/bin/env python3
"""
ä½¿ç”¨ Gemini 2.5 Flash ä¼˜åŒ–æ–‡æ¡£ï¼ˆå¤„ç†è¶…é•¿æ–‡æœ¬ï¼‰
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scripts.optimize.gemini_document_optimizer import GeminiDocumentOptimizer, OptimizationConfig

def optimize_document(input_file, output_file):
    """ä¼˜åŒ–æ–‡æ¡£"""

    # APIå¯†é’¥
    API_KEY = "AIzaSyDAmHzFbKgZMJVp1nyOkVr89MjXM6ahWnE"

    print("=" * 60)
    print("Gemini 2.5 Flash æ–‡æ¡£ä¼˜åŒ–")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = OptimizationConfig(
        api_key=API_KEY,
        model_name='models/gemini-2.5-flash',
        temperature=0.3,
        cache_enabled=True,
        max_tokens_per_request=10000  # å‡å°æ¯æ¬¡è¯·æ±‚çš„å¤§å°
    )

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = GeminiDocumentOptimizer(config)

    # è¯»å–æ–‡ä»¶
    print(f"\nğŸ“– è¯»å–æ–‡ä»¶: {input_file}")
    content = Path(input_file).read_text(encoding='utf-8')
    print(f"æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")

    # å…ˆè¿›è¡ŒåŸºç¡€çº é”™
    print("\nğŸ”§ ç¬¬ä¸€æ­¥ï¼šåŸºç¡€çº é”™...")
    corrected = optimizer.apply_term_corrections(content)

    # æˆªå–å‰é¢éƒ¨åˆ†è¿›è¡ŒAIä¼˜åŒ–ï¼ˆé¿å…è¶…æ—¶ï¼‰
    print("\nğŸ¤– ç¬¬äºŒæ­¥ï¼šAIæ·±åº¦ä¼˜åŒ–ï¼ˆå¤„ç†å‰5000å­—ï¼‰...")

    # åªå¤„ç†å‰5000å­—ç¬¦
    sample_text = corrected[:5000]

    try:
        # åˆ†æ®µä¼˜åŒ–
        chunks = optimizer.split_text(sample_text, max_length=2000)
        optimized_parts = []

        for i, chunk in enumerate(chunks):
            print(f"  å¤„ç†ç¬¬ {i+1}/{len(chunks)} æ®µ...")

            # ä½¿ç”¨æ›´ç›´æ¥çš„æç¤º
            import google.generativeai as genai
            genai.configure(api_key=API_KEY)
            model = genai.GenerativeModel('models/gemini-2.5-flash')

            prompt = f"""
è¯·ä¼˜åŒ–ä»¥ä¸‹è¯­éŸ³è½¬å½•æ–‡æœ¬ï¼Œä½¿å…¶æ›´ä¸“ä¸šã€ç»“æ„åŒ–ï¼š

1. ä¿æŒåŸæ„ä¸å˜
2. æ”¹å–„æ®µè½ç»“æ„
3. å»é™¤å£è¯­åŒ–è¡¨è¾¾
4. ä½¿ç”¨Markdownæ ¼å¼

åŸæ–‡ï¼š
{chunk}

ä¼˜åŒ–åçš„æ–‡æœ¬ï¼š
"""

            response = model.generate_content(prompt)
            optimized_parts.append(response.text)

            # é¿å…APIé™æµ
            if i < len(chunks) - 1:
                time.sleep(2)

        # åˆå¹¶ç»“æœ
        optimized_content = "\n\n".join(optimized_parts)

        # æ·»åŠ å‰©ä½™çš„çº é”™å†…å®¹ï¼ˆå¦‚æœæ–‡æ¡£å¾ˆé•¿ï¼‰
        if len(corrected) > 5000:
            optimized_content += "\n\n[ä»¥ä¸‹ä¸ºåŸºç¡€çº é”™å†…å®¹]\n\n" + corrected[5000:]

        # ä¿å­˜ç»“æœ
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        Path(output_file).write_text(optimized_content, encoding='utf-8')

        print(f"\nâœ… ä¼˜åŒ–å®Œæˆï¼")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

        # æ˜¾ç¤ºé¢„è§ˆ
        print("\nğŸ“„ ä¼˜åŒ–ç»“æœé¢„è§ˆï¼ˆå‰500å­—ï¼‰ï¼š")
        print("-" * 50)
        print(optimized_content[:500])
        print("-" * 50)

        return True

    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–å¤±è´¥: {e}")

        # è‡³å°‘ä¿å­˜çº é”™ç‰ˆæœ¬
        fallback_file = output_file.replace('.md', '_corrected_only.md')
        Path(fallback_file).parent.mkdir(parents=True, exist_ok=True)
        Path(fallback_file).write_text(corrected, encoding='utf-8')
        print(f"\nğŸ’¾ å·²ä¿å­˜åŸºç¡€çº é”™ç‰ˆæœ¬: {fallback_file}")

        return False


if __name__ == "__main__":
    # è¾“å…¥è¾“å‡ºæ–‡ä»¶
    input_file = "storage/results/expert_optimized/å››å¤§æ¨ç†å¤§æ¨¡å‹æ•°å­¦ä¸ç¼–ç¨‹èƒ½åŠ›è¯„æµ‹ - Grok3ã€Claude3.7ã€DeepSeep-R1ã€o3-mini-high åˆ°åº•è°çš„æ¨ç†èƒ½åŠ›æœ€å¼ºï¼Ÿ_æ·±åº¦ä¼˜åŒ–ç‰ˆ.md"
    output_file = "storage/results/gemini_optimized/å››å¤§æ¨ç†å¤§æ¨¡å‹è¯„æµ‹_gemini25_final.md"

    success = optimize_document(input_file, output_file)

    if success:
        print("\nğŸ‰ æ–‡æ¡£ä¼˜åŒ–æˆåŠŸ!")
    else:
        print("\nâš ï¸ éƒ¨åˆ†ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¾“å‡º")