#!/usr/bin/env python3
"""
æ‰¹é‡ä¼˜åŒ–æ‰€æœ‰æ–‡æ¡£ - ä½¿ç”¨ Gemini 2.5 Flash
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scripts.optimize.gemini_document_optimizer import GeminiDocumentOptimizer, OptimizationConfig
import google.generativeai as genai

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_optimization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
API_KEY = "AIzaSyDAmHzFbKgZMJVp1nyOkVr89MjXM6ahWnE"


class BatchDocumentOptimizer:
    """æ‰¹é‡æ–‡æ¡£ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.config = OptimizationConfig(
            api_key=API_KEY,
            model_name='models/gemini-2.5-flash',
            temperature=0.3,
            cache_enabled=True,
            max_tokens_per_request=8000  # é€‚ä¸­çš„è¯·æ±‚å¤§å°
        )
        self.optimizer = GeminiDocumentOptimizer(self.config)
        genai.configure(api_key=API_KEY)
        self.model = genai.GenerativeModel('models/gemini-2.5-flash')

        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }

        # è¿›åº¦æ–‡ä»¶
        self.progress_file = Path("batch_progress.json")
        self.load_progress()

    def load_progress(self):
        """åŠ è½½è¿›åº¦"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                self.progress = json.load(f)
        else:
            self.progress = {'completed': [], 'failed': []}

    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.progress, f, ensure_ascii=False, indent=2)

    def find_documents(self, input_dir: str) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰å¾…ä¼˜åŒ–æ–‡æ¡£"""
        input_path = Path(input_dir)
        files = list(input_path.glob("*.md"))

        # è¿‡æ»¤å·²å®Œæˆçš„æ–‡ä»¶
        files = [f for f in files if str(f) not in self.progress['completed']]

        return sorted(files)

    def optimize_single_document(self, input_file: Path, output_dir: Path) -> bool:
        """ä¼˜åŒ–å•ä¸ªæ–‡æ¡£"""
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_name = input_file.stem.replace('_æ·±åº¦ä¼˜åŒ–ç‰ˆ', '').replace('_ä¸“å®¶ä¼˜åŒ–ç‰ˆ', '')
            output_name = f"{output_name}_gemini25_optimized.md"
            output_file = output_dir / output_name

            logger.info(f"å¼€å§‹ä¼˜åŒ–: {input_file.name}")

            # è¯»å–æ–‡ä»¶
            content = input_file.read_text(encoding='utf-8')
            file_size = len(content)

            # 1. åŸºç¡€çº é”™
            logger.info("  ç¬¬1æ­¥ï¼šåŸºç¡€çº é”™...")
            corrected = self.optimizer.apply_term_corrections(content)

            # 2. æ™ºèƒ½ä¼˜åŒ–ï¼ˆåˆ†æ®µå¤„ç†å¤§æ–‡ä»¶ï¼‰
            logger.info("  ç¬¬2æ­¥ï¼šAIæ·±åº¦ä¼˜åŒ–...")

            if file_size > 10000:
                # å¤§æ–‡ä»¶ï¼šåªå¤„ç†å‰8000å­—ç¬¦
                sample_size = 8000
                sample_text = corrected[:sample_size]
                logger.info(f"  æ–‡ä»¶è¾ƒå¤§({file_size}å­—ç¬¦)ï¼Œä¼˜åŒ–å‰{sample_size}å­—ç¬¦")
            else:
                # å°æ–‡ä»¶ï¼šå…¨éƒ¨å¤„ç†
                sample_text = corrected
                sample_size = file_size

            # åˆ†æ®µå¤„ç†
            chunks = self.optimizer.split_text(sample_text, max_length=3000)
            optimized_parts = []

            for i, chunk in enumerate(chunks):
                logger.info(f"    å¤„ç†æ®µè½ {i+1}/{len(chunks)}...")

                prompt = f"""
è¯·å°†ä»¥ä¸‹è¯­éŸ³è½¬å½•æ–‡æœ¬ä¼˜åŒ–ä¸ºä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ï¼š

è¦æ±‚ï¼š
1. çº æ­£æ‰€æœ‰ä¸“æœ‰åè¯é”™è¯¯ï¼ˆå¦‚AIæ¨¡å‹åç§°ã€æŠ€æœ¯æœ¯è¯­ï¼‰
2. æ”¹å–„æ®µè½ç»“æ„ï¼Œæ·»åŠ åˆé€‚çš„æ ‡é¢˜å±‚çº§
3. å»é™¤å£è¯­åŒ–è¡¨è¾¾å’Œé‡å¤å†…å®¹
4. ä¿æŒåŸæ„ä¸å˜ï¼Œä½†ä½¿è¡¨è¾¾æ›´ä¸“ä¸š
5. ä½¿ç”¨Markdownæ ¼å¼

åŸæ–‡ï¼š
{chunk}

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•è¯´æ˜ï¼š
"""

                try:
                    response = self.model.generate_content(prompt)
                    optimized_parts.append(response.text)

                    # é¿å…APIé™æµ
                    if i < len(chunks) - 1:
                        time.sleep(2)

                except Exception as e:
                    logger.warning(f"    æ®µè½{i+1}ä¼˜åŒ–å¤±è´¥: {e}")
                    # é™çº§ï¼šä½¿ç”¨çº é”™ç‰ˆæœ¬
                    optimized_parts.append(chunk)

            # åˆå¹¶ç»“æœ
            optimized_content = "\n\n".join(optimized_parts)

            # å¦‚æœæ–‡æ¡£å¾ˆé•¿ï¼Œæ·»åŠ å‰©ä½™çš„çº é”™å†…å®¹
            if file_size > sample_size:
                optimized_content += f"\n\n---\n\n_[æ³¨ï¼šä»¥ä¸‹ä¸ºåŸºç¡€çº é”™å†…å®¹ï¼Œæœªç»AIæ·±åº¦ä¼˜åŒ–]_\n\n"
                optimized_content += corrected[sample_size:]

            # ä¿å­˜ç»“æœ
            output_file.parent.mkdir(parents=True, exist_ok=True)
            output_file.write_text(optimized_content, encoding='utf-8')

            logger.info(f"  âœ… æˆåŠŸï¼è¾“å‡º: {output_file.name}")

            # è®°å½•è¿›åº¦
            self.progress['completed'].append(str(input_file))
            self.save_progress()

            self.stats['success'] += 1
            return True

        except Exception as e:
            logger.error(f"  âŒ å¤±è´¥: {e}")
            self.progress['failed'].append(str(input_file))
            self.save_progress()
            self.stats['failed'] += 1
            return False

    def optimize_all(self, input_dir: str, output_dir: str):
        """æ‰¹é‡ä¼˜åŒ–æ‰€æœ‰æ–‡æ¡£"""
        # æŸ¥æ‰¾æ–‡æ¡£
        files = self.find_documents(input_dir)
        self.stats['total'] = len(files)

        if not files:
            logger.info("æ²¡æœ‰å¾…ä¼˜åŒ–çš„æ–‡æ¡£ï¼ˆå¯èƒ½éƒ½å·²å®Œæˆï¼‰")
            return

        logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªå¾…ä¼˜åŒ–æ–‡æ¡£")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        for idx, file in enumerate(files, 1):
            logger.info(f"\n[{idx}/{len(files)}] å¤„ç†æ–‡æ¡£...")

            # ä¼˜åŒ–æ–‡æ¡£
            success = self.optimize_single_document(file, output_path)

            # æ˜¾ç¤ºè¿›åº¦
            progress_pct = (self.stats['success'] + self.stats['failed']) / self.stats['total'] * 100
            logger.info(f"æ€»è¿›åº¦: {progress_pct:.1f}% "
                       f"(æˆåŠŸ: {self.stats['success']}, å¤±è´¥: {self.stats['failed']})")

            # ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…APIé™æµ
            if idx < len(files):
                wait_time = 3
                logger.info(f"ç­‰å¾…{wait_time}ç§’...")
                time.sleep(wait_time)

    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("æ‰¹é‡ä¼˜åŒ–å®Œæˆ")
        logger.info("="*60)
        logger.info(f"æ€»æ–‡æ¡£æ•°: {self.stats['total']}")
        logger.info(f"æˆåŠŸ: {self.stats['success']}")
        logger.info(f"å¤±è´¥: {self.stats['failed']}")
        logger.info(f"è·³è¿‡: {self.stats['skipped']}")

        if self.stats['failed'] > 0:
            logger.info("\nå¤±è´¥çš„æ–‡æ¡£:")
            for doc in self.progress['failed']:
                logger.info(f"  - {Path(doc).name}")

        # æ¸…ç†è¿›åº¦æ–‡ä»¶ï¼ˆå¦‚æœå…¨éƒ¨æˆåŠŸï¼‰
        if self.stats['failed'] == 0 and self.stats['success'] == self.stats['total']:
            self.progress_file.unlink(missing_ok=True)
            logger.info("\nâœ… æ‰€æœ‰æ–‡æ¡£ä¼˜åŒ–æˆåŠŸï¼Œå·²æ¸…ç†è¿›åº¦æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    input_dir = "storage/results/expert_optimized"
    output_dir = "storage/results/gemini_optimized"

    logger.info("="*60)
    logger.info("Gemini 2.5 Flash æ‰¹é‡æ–‡æ¡£ä¼˜åŒ–")
    logger.info("="*60)
    logger.info(f"è¾“å…¥ç›®å½•: {input_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"APIæ¨¡å‹: models/gemini-2.5-flash")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = BatchDocumentOptimizer()

    try:
        # æ‰§è¡Œæ‰¹é‡ä¼˜åŒ–
        optimizer.optimize_all(input_dir, output_dir)

        # æ‰“å°æ‘˜è¦
        optimizer.print_summary()

        # ç”ŸæˆæŠ¥å‘Š
        report_file = Path(output_dir) / "optimization_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"Gemini 2.5 Flash æ‰¹é‡ä¼˜åŒ–æŠ¥å‘Š\n")
            f.write(f"{'='*50}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ–‡æ¡£æ•°: {optimizer.stats['total']}\n")
            f.write(f"æˆåŠŸ: {optimizer.stats['success']}\n")
            f.write(f"å¤±è´¥: {optimizer.stats['failed']}\n")

        logger.info(f"\nğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

    except KeyboardInterrupt:
        logger.info("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
        optimizer.save_progress()
        logger.info("è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ç»§ç»­")
        return 1

    except Exception as e:
        logger.error(f"\næ‰¹é‡ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == '__main__':
    exit(main())