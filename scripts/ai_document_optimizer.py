#!/usr/bin/env python3
"""
AIé©±åŠ¨çš„æ–‡æ¡£æ·±åº¦ä¼˜åŒ–å™¨
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç†è§£å’Œé‡æ„æŠ€æœ¯æ–‡æ¡£
"""

import os
import sys
from pathlib import Path
import json
import re
from typing import Dict, List, Tuple

# ä¸“ä¸šæœ¯è¯­ä¿®æ­£å­—å…¸ï¼ˆæ‰©å±•ç‰ˆï¼‰
TECH_TERMS = {
    # AI/MLæœ¯è¯­
    'Agent': ['agent', 'Agent', 'AGENT', 'è‰¾çœŸç‰¹', 'çˆ±è¿‘ç‰¹', 'H'],
    'Token': ['token', 'Token', 'å·æ‡‡', 'æŠ•è‚¯', 'æ‹–è‚¯'],
    'Context': ['context', 'Context', 'CONTEXT', 'åº·ç‰¹æ–¯ç‰¹'],
    'Context Window': ['context window', 'Context Window', 'ä¸Šä¸‹æ–‡çª—å£', 'ä¸Šä¸‹è¼ªçª—å£'],
    'Context Engineering': ['Context Engineering', 'Context and Engineering', 'ä¸Šä¸‹æ–‡å·¥ç¨‹'],

    # åè®®ç›¸å…³
    'A2A': ['A2A', 'a2a', 'Agent to Agent', 'agent to agent'],
    'MCP': ['MCP', 'mcp', 'Model Context Protocol'],
    'Function Calling': ['Function Calling', 'function calling', 'æ–¹å…‹æ£®é 0', 'å‡½æ•°è°ƒç”¨'],
    'JSON-RPC': ['JSON-RPC', 'JSAN RPC', 'JSYRPC', 'JSON RPC'],

    # æ¨¡å‹ç›¸å…³
    'GPT': ['GPT', 'gpt', 'GPD', 'GPTD'],
    'Claude': ['Claude', 'Cloud', 'CLOUD', 'å…‹åŠ³å¾·'],
    'ChatGPT': ['ChatGPT', 'Chaggbt', 'Chat GPT'],
    'Gemini': ['Gemini', 'GEMON', 'JAMMDR', 'Gemma'],
    'DeepSeek': ['DeepSeek', 'DipSync', 'Deepseek'],

    # æŠ€æœ¯æ¦‚å¿µ
    'RAG': ['RAG', 'rag', 'Rag', 'æ£€ç´¢å¢å¼ºç”Ÿæˆ'],
    'Artifact': ['Artifact', 'artifact', 'RDFACT', 'Artifacts'],
    'Multi-Agent': ['Multi-Agent', 'multi-agent', 'MALSEAZEN', 'Mouth Agents', 'MaltzAgent'],
    'streaming': ['streaming', 'SREEM', 'stream', 'æµå¼', 'æµæ°'],

    # å¼€å‘ç›¸å…³
    'API': ['API', 'api', 'API key', 'API Key'],
    'SDK': ['SDK', 'sdk'],
    'HTTP': ['HTTP', 'HTDP', 'HDDP', 'http'],
    'URL': ['URL', 'url', 'UIR', 'UR'],
    'localhost': ['localhost', 'Local Host', 'local host'],

    # ä¸­æ–‡æœ¯è¯­ä¿®æ­£
    'è¾“å…¥': ['è¾“å…¥', 'æ›¸å…¥', 'ä¹¦å…¥'],
    'è¾“å‡º': ['è¾“å‡º', 'è¼¸å‡º'],
    'è¿”å›': ['è¿”å›', 'åå›', 'ç‰ˆå›', 'è¿”å›'],
    'è°ƒç”¨': ['è°ƒç”¨', 'é›»å½±', 'ç”µå½±'],
    'å‡½æ•°': ['å‡½æ•°', 'éŸ“æ•¸', 'å¯’æ•°'],
    'åè®®': ['åè®®', 'å”è­°', 'HUAåè®®', 'HAAåè®®'],
    'å·¥å…·': ['å·¥å…·', 'FCPå·¥å…·', 'MCPå·¥å…·'],

    # å…¬å¸å’Œå·¥å…·
    'Google': ['Google', 'google', 'è°·æ­Œ'],
    'Anthropic': ['Anthropic', 'Anthorapic'],
    'OpenAI': ['OpenAI', 'openai', 'Open AI'],
    'Cline': ['Cline', 'Colline', 'å…‹è±æ©'],
    'Cursor': ['Cursor', 'cursor', 'å…‰æ ‡'],
    'Wireshark': ['Wireshark', 'WarShark', 'wareshark'],
    'LangChain': ['LangChain', 'ç‹¼Cin', 'langchain'],

    # å…¶ä»–æœ¯è¯­
    'WebSocket': ['WebSocket', 'websocket', 'Web Socket'],
    'Base64': ['Base64', 'base64'],
    'PNG': ['PNG', 'png', 'image gun png'],
    'JSON': ['JSON', 'json', 'JSAN', 'éä¸Š'],
}

def fix_technical_terms(text: str) -> str:
    """ä¿®æ­£æŠ€æœ¯æœ¯è¯­"""
    for correct_term, variations in TECH_TERMS.items():
        for variant in variations:
            if variant != correct_term:
                # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                pattern = r'\b' + re.escape(variant) + r'\b'
                text = re.sub(pattern, correct_term, text, flags=re.IGNORECASE)
    return text

def fix_punctuation(text: str) -> str:
    """ä¿®æ­£æ ‡ç‚¹ç¬¦å·"""
    # ä¿®æ­£ä¸­æ–‡æ ‡ç‚¹
    text = text.replace('ï¼Œ', 'ï¼Œ')
    text = text.replace('ã€‚', 'ã€‚')
    text = text.replace('ï¼Ÿ', 'ï¼Ÿ')
    text = text.replace('ï¼', 'ï¼')
    text = text.replace('ï¼š', 'ï¼š')
    text = text.replace('ï¼›', 'ï¼›')
    text = text.replace('"', '"')
    text = text.replace('"', '"')
    text = text.replace(''', ''')
    text = text.replace(''', ''')

    # åœ¨å¥å·åæ·»åŠ æ¢è¡Œä»¥æ”¹å–„å¯è¯»æ€§
    text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([^ã€‚ï¼ï¼Ÿ\n])', r'\1\n\n\2', text)

    return text

def identify_sections(text: str) -> List[Tuple[str, str]]:
    """è¯†åˆ«æ–‡æ¡£çš„ä¸»è¦ç« èŠ‚"""
    sections = []

    # å®šä¹‰ç« èŠ‚è¯†åˆ«æ¨¡å¼
    patterns = {
        'æ¦‚è¿°': [r'é¦–å…ˆ|ä»€ä¹ˆæ˜¯|ç®€å•æ¥è¯´|æ¦‚å¿µ|ä»‹ç»|å¼•è¨€'],
        'èƒŒæ™¯': [r'èƒŒæ™¯|åŸå› |ä¸ºä»€ä¹ˆ|é—®é¢˜æ˜¯|ç°çŠ¶'],
        'æ ¸å¿ƒæ¦‚å¿µ': [r'æ ¸å¿ƒ|å…³é”®|é‡è¦|ä¸»è¦|åŸºæœ¬'],
        'åŸç†': [r'åŸç†|å·¥ä½œæœºåˆ¶|å¦‚ä½•å·¥ä½œ|å®ç°æ–¹å¼|æœºåˆ¶'],
        'æ¶æ„': [r'æ¶æ„|ç»“æ„|ç»„æˆ|æ¨¡å—|ç»„ä»¶'],
        'å®ç°': [r'å®ç°|ä»£ç |æ­¥éª¤|æ–¹æ³•|æŠ€æœ¯|æµç¨‹'],
        'ä½¿ç”¨åœºæ™¯': [r'åœºæ™¯|ä¾‹å­|æ¡ˆä¾‹|ä½¿ç”¨|åº”ç”¨'],
        'å¯¹æ¯”åˆ†æ': [r'å¯¹æ¯”|åŒºåˆ«|ä¸åŒ|ç›¸æ¯”|æ¯”è¾ƒ'],
        'ä¼˜ç¼ºç‚¹': [r'ä¼˜ç‚¹|ç¼ºç‚¹|ä¼˜åŠ¿|åŠ£åŠ¿|å¥½å¤„|é—®é¢˜'],
        'æœ€ä½³å®è·µ': [r'æœ€ä½³å®è·µ|å»ºè®®|æ¨è|æŠ€å·§|æ³¨æ„'],
        'æ€»ç»“': [r'æ€»ç»“|æ€»çš„æ¥è¯´|æœ€å|ç»“è®º|å›é¡¾'],
        'å‚è€ƒèµ„æº': [r'å‚è€ƒ|èµ„æº|æ–‡æ¡£|é“¾æ¥|å­¦ä¹ '],
    }

    paragraphs = text.split('\n\n')
    current_section = 'å¼•è¨€'
    current_content = []

    for para in paragraphs:
        section_found = False
        for section_name, section_patterns in patterns.items():
            for pattern in section_patterns:
                if re.search(pattern, para[:100], re.IGNORECASE):
                    if current_content:
                        sections.append((current_section, '\n\n'.join(current_content)))
                    current_section = section_name
                    current_content = [para]
                    section_found = True
                    break
            if section_found:
                break

        if not section_found:
            current_content.append(para)

    if current_content:
        sections.append((current_section, '\n\n'.join(current_content)))

    return sections

def extract_code_blocks(text: str) -> str:
    """è¯†åˆ«å¹¶æ ¼å¼åŒ–ä»£ç å—"""
    # è¯†åˆ«å‘½ä»¤è¡Œå‘½ä»¤
    text = re.sub(r'((?:npm|pip|conda|python|node|git|docker|kubectl)\s+[^\n]+)',
                  r'```bash\n\1\n```', text)

    # è¯†åˆ«è·¯å¾„
    text = re.sub(r'((?:/[a-zA-Z0-9_\-]+)+(?:/[a-zA-Z0-9_\-\.]+)?)',
                  r'`\1`', text)

    # è¯†åˆ«URL
    text = re.sub(r'(https?://[^\s]+)', r'[\1](\1)', text)

    return text

def create_structured_markdown(title: str, sections: List[Tuple[str, str]]) -> str:
    """åˆ›å»ºç»“æ„åŒ–çš„Markdownæ–‡æ¡£"""
    md = f"# {title}\n\n"

    # æ·»åŠ ç›®å½•
    if len(sections) > 3:
        md += "## ç›®å½•\n\n"
        for section_name, _ in sections:
            md += f"- [{section_name}](#{section_name.lower().replace(' ', '-')})\n"
        md += "\n---\n\n"

    # æ·»åŠ å„ä¸ªç« èŠ‚
    for section_name, content in sections:
        md += f"## {section_name}\n\n"

        # å¤„ç†å†…å®¹
        content = fix_technical_terms(content)
        content = fix_punctuation(content)
        content = extract_code_blocks(content)

        # æ™ºèƒ½åˆ†æ®µ
        paragraphs = content.split('\n\n')
        for para in paragraphs:
            if len(para) > 500:
                # é•¿æ®µè½å°è¯•è¿›ä¸€æ­¥åˆ†å‰²
                sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', para)
                new_para = ""
                current_len = 0
                for i in range(0, len(sentences)-1, 2):
                    sentence = sentences[i] + sentences[i+1] if i+1 < len(sentences) else sentences[i]
                    new_para += sentence
                    current_len += len(sentence)
                    if current_len > 200 and i < len(sentences)-2:
                        new_para += "\n\n"
                        current_len = 0
                md += new_para + "\n\n"
            else:
                md += para + "\n\n"

    return md

def optimize_document(input_path: Path, output_path: Path) -> bool:
    """ä¼˜åŒ–å•ä¸ªæ–‡æ¡£"""
    try:
        # è¯»å–åŸå§‹æ–‡æœ¬
        with open(input_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # æå–æ ‡é¢˜
        lines = text.split('\n')
        if lines[0].startswith('# '):
            title = lines[0].replace('# ', '').strip()
            text = '\n'.join(lines[1:])
        else:
            title = input_path.stem

        # è¯†åˆ«ç« èŠ‚
        sections = identify_sections(text)

        # åˆ›å»ºç»“æ„åŒ–Markdown
        optimized_md = create_structured_markdown(title, sections)

        # ä¿å­˜ä¼˜åŒ–åçš„æ–‡æ¡£
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(optimized_md)

        print(f"âœ… ä¼˜åŒ–å®Œæˆ: {output_path.name}")
        return True

    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å¤±è´¥ {input_path.name}: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    input_dir = Path("storage/results/mark_transcripts/markdown")
    output_dir = Path("storage/results/mark_transcripts/optimized")
    output_dir.mkdir(exist_ok=True)

    # è·å–æ‰€æœ‰markdownæ–‡ä»¶
    md_files = list(input_dir.glob("*.md"))
    print(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£å¾…ä¼˜åŒ–")

    success_count = 0
    for md_file in md_files:
        output_file = output_dir / md_file.name
        if optimize_document(md_file, output_file):
            success_count += 1

    print(f"\nğŸ“Š ä¼˜åŒ–å®Œæˆç»Ÿè®¡ï¼š")
    print(f"   æˆåŠŸ: {success_count}/{len(md_files)}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")

if __name__ == "__main__":
    main()