#!/usr/bin/env python3
"""
æŠ€æœ¯æ–‡æ¡£ä¼˜åŒ–æµæ°´çº¿

é›†æˆæ‰€æœ‰ä¼˜åŒ–æ­¥éª¤ï¼Œæä¾›å®Œæ•´çš„æ–‡æ¡£å¤„ç†æ–¹æ¡ˆ
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from scripts.optimize.document_optimizer import DocumentOptimizer
from scripts.optimize.structure_generator import TechnicalDocumentEnhancer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/dministrator/project/Bili2Text/logs/optimization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizationPipeline:
    """æ–‡æ¡£ä¼˜åŒ–æµæ°´çº¿"""

    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.document_optimizer = DocumentOptimizer()
        self.document_enhancer = TechnicalDocumentEnhancer()
        self.stats = {
            'processed': 0,
            'succeeded': 0,
            'failed': 0,
            'errors': []
        }

    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            "input_dir": "/home/dministrator/project/Bili2Text/storage/results/mark_transcripts/markdown",
            "temp_dir": "/home/dministrator/project/Bili2Text/storage/temp/optimization",
            "output_dir": "/home/dministrator/project/Bili2Text/storage/results/optimized_transcripts",
            "backup_dir": "/home/dministrator/project/Bili2Text/storage/backup/original_transcripts",
            "file_patterns": ["*.md"],
            "skip_patterns": ["optimized_*", "enhanced_*"],
            "optimization_stages": [
                "term_correction",
                "structure_analysis",
                "content_enhancement",
                "technical_validation"
            ],
            "quality_checks": {
                "min_paragraphs": 3,
                "min_sections": 2,
                "max_paragraph_length": 1000
            }
        }

        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)

        return default_config

    def run_pipeline(self, files: Optional[List[str]] = None) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æµæ°´çº¿"""
        logger.info("å¼€å§‹æŠ€æœ¯æ–‡æ¡£ä¼˜åŒ–æµæ°´çº¿")

        try:
            # å‡†å¤‡å·¥ä½œç›®å½•
            self._prepare_directories()

            # è·å–å¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨
            file_list = files or self._get_file_list()

            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            for file_path in file_list:
                self._process_single_file(file_path)

            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report()

            logger.info(f"ä¼˜åŒ–æµæ°´çº¿å®Œæˆ: {self.stats['succeeded']}/{self.stats['processed']} æ–‡ä»¶æˆåŠŸå¤„ç†")
            return report

        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def _prepare_directories(self):
        """å‡†å¤‡å·¥ä½œç›®å½•"""
        for dir_key in ['temp_dir', 'output_dir', 'backup_dir']:
            dir_path = Path(self.config[dir_key])
            dir_path.mkdir(parents=True, exist_ok=True)

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = Path("/home/dministrator/project/Bili2Text/logs")
        log_dir.mkdir(parents=True, exist_ok=True)

    def _get_file_list(self) -> List[str]:
        """è·å–å¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        input_dir = Path(self.config['input_dir'])
        file_list = []

        for pattern in self.config['file_patterns']:
            files = input_dir.glob(pattern)
            for file_path in files:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                skip = False
                for skip_pattern in self.config['skip_patterns']:
                    if skip_pattern in file_path.name:
                        skip = True
                        break

                if not skip:
                    file_list.append(str(file_path))

        logger.info(f"æ‰¾åˆ° {len(file_list)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        return file_list

    def _process_single_file(self, file_path: str):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        self.stats['processed'] += 1
        file_name = Path(file_path).name

        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}")

            # å¤‡ä»½åŸå§‹æ–‡ä»¶
            self._backup_file(file_path)

            # ç¬¬ä¸€é˜¶æ®µ: åŸºç¡€ä¼˜åŒ– (æœ¯è¯­ä¿®æ­£ã€æ®µè½ä¼˜åŒ–)
            temp_file1 = Path(self.config['temp_dir']) / f"stage1_{file_name}"
            success1 = self.document_optimizer.optimize_document(file_path, str(temp_file1))

            if not success1:
                raise Exception("åŸºç¡€ä¼˜åŒ–é˜¶æ®µå¤±è´¥")

            # ç¬¬äºŒé˜¶æ®µ: ç»“æ„å¢å¼º
            temp_file2 = Path(self.config['temp_dir']) / f"stage2_{file_name}"
            success2 = self.document_enhancer.enhance_document(str(temp_file1), str(temp_file2))

            if not success2:
                raise Exception("ç»“æ„å¢å¼ºé˜¶æ®µå¤±è´¥")

            # ç¬¬ä¸‰é˜¶æ®µ: è´¨é‡æ£€æŸ¥å’Œæœ€ç»ˆè¾“å‡º
            output_file = Path(self.config['output_dir']) / f"optimized_{file_name}"
            success3 = self._final_quality_check(str(temp_file2), str(output_file))

            if not success3:
                raise Exception("è´¨é‡æ£€æŸ¥é˜¶æ®µå¤±è´¥")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_file1.unlink(missing_ok=True)
            temp_file2.unlink(missing_ok=True)

            self.stats['succeeded'] += 1
            logger.info(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_name}")

        except Exception as e:
            self.stats['failed'] += 1
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}"
            self.stats['errors'].append(error_msg)
            logger.error(error_msg)

    def _backup_file(self, file_path: str):
        """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
        try:
            source = Path(file_path)
            backup_dir = Path(self.config['backup_dir'])
            backup_file = backup_dir / source.name

            if not backup_file.exists():
                import shutil
                shutil.copy2(source, backup_file)
                logger.debug(f"å·²å¤‡ä»½: {source.name}")

        except Exception as e:
            logger.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")

    def _final_quality_check(self, input_file: str, output_file: str) -> bool:
        """æœ€ç»ˆè´¨é‡æ£€æŸ¥"""
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŸºæœ¬è´¨é‡æ£€æŸ¥
            checks = self.config.get('quality_checks', {
                'min_paragraphs': 3,
                'min_sections': 2,
                'max_paragraph_length': 1000
            })

            # æ£€æŸ¥æ®µè½æ•°é‡
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            if len(paragraphs) < checks['min_paragraphs']:
                logger.warning(f"æ®µè½æ•°é‡ä¸è¶³: {len(paragraphs)} < {checks['min_paragraphs']}")

            # æ£€æŸ¥ç« èŠ‚æ•°é‡
            sections = len([line for line in content.split('\n') if line.startswith('#')])
            if sections < checks['min_sections']:
                logger.warning(f"ç« èŠ‚æ•°é‡ä¸è¶³: {sections} < {checks['min_sections']}")

            # æ£€æŸ¥æ®µè½é•¿åº¦
            for i, para in enumerate(paragraphs):
                if len(para) > checks['max_paragraph_length']:
                    logger.warning(f"æ®µè½ {i+1} è¿‡é•¿: {len(para)} > {checks['max_paragraph_length']}")

            # æ·»åŠ ä¼˜åŒ–è¯´æ˜
            enhanced_content = self._add_optimization_notes(content)

            # å†™å…¥æœ€ç»ˆæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(enhanced_content)

            return True

        except Exception as e:
            logger.error(f"è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _add_optimization_notes(self, content: str) -> str:
        """æ·»åŠ ä¼˜åŒ–è¯´æ˜"""
        optimization_note = """
---

## æ–‡æ¡£ä¼˜åŒ–è¯´æ˜

æœ¬æ–‡æ¡£å·²é€šè¿‡ä»¥ä¸‹ä¼˜åŒ–æ­¥éª¤å¤„ç†ï¼š

1. **æœ¯è¯­ä¿®æ­£**: ä¿®æ­£äº†è¯­éŸ³è¯†åˆ«äº§ç”Ÿçš„æŠ€æœ¯æœ¯è¯­é”™è¯¯
2. **ç»“æ„ä¼˜åŒ–**: é‡æ–°ç»„ç»‡äº†æ–‡æ¡£å±‚æ¬¡ç»“æ„
3. **å†…å®¹å¢å¼º**: æ”¹å–„äº†æ®µè½åˆ’åˆ†å’Œé€»è¾‘ç»“æ„
4. **è´¨é‡æ£€æŸ¥**: ç¡®ä¿äº†æ–‡æ¡£çš„æŠ€æœ¯å‡†ç¡®æ€§å’Œå¯è¯»æ€§

> ğŸ’¡ æç¤ºï¼šå¦‚å‘ç°ä»»ä½•æŠ€æœ¯é”™è¯¯æˆ–éœ€è¦è¿›ä¸€æ­¥è¯´æ˜çš„å†…å®¹ï¼Œè¯·å‚è€ƒåŸå§‹è½¬å½•æ–‡ä»¶æˆ–ç›¸å…³æŠ€æœ¯æ–‡æ¡£ã€‚

---
"""
        return content + optimization_note

    def _generate_report(self) -> Dict:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = {
            'summary': {
                'total_processed': self.stats['processed'],
                'successful': self.stats['succeeded'],
                'failed': self.stats['failed'],
                'success_rate': f"{(self.stats['succeeded'] / max(self.stats['processed'], 1)) * 100:.1f}%"
            },
            'errors': self.stats['errors'],
            'output_directory': self.config['output_dir'],
            'backup_directory': self.config['backup_dir']
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(self.config['output_dir']) / 'optimization_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        return report

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æŠ€æœ¯æ–‡æ¡£ä¼˜åŒ–æµæ°´çº¿')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--files', nargs='+', help='æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ˜¾ç¤ºå°†è¦å¤„ç†çš„æ–‡ä»¶ï¼Œä¸æ‰§è¡Œä¼˜åŒ–')

    args = parser.parse_args()

    try:
        pipeline = OptimizationPipeline(args.config)

        if args.dry_run:
            file_list = pipeline._get_file_list()
            print(f"å°†è¦å¤„ç†çš„æ–‡ä»¶ ({len(file_list)}):")
            for file_path in file_list:
                print(f"  - {file_path}")
            return

        report = pipeline.run_pipeline(args.files)

        print("\n=== ä¼˜åŒ–æŠ¥å‘Š ===")
        print(f"æ€»å¤„ç†æ–‡ä»¶: {report['summary']['total_processed']}")
        print(f"æˆåŠŸ: {report['summary']['successful']}")
        print(f"å¤±è´¥: {report['summary']['failed']}")
        print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")

        if report['errors']:
            print(f"\né”™è¯¯åˆ—è¡¨:")
            for error in report['errors']:
                print(f"  - {error}")

        print(f"\nè¾“å‡ºç›®å½•: {report['output_directory']}")
        print(f"å¤‡ä»½ç›®å½•: {report['backup_directory']}")

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()