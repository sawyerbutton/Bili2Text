#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æ¡£æ·±åº¦ä¼˜åŒ–ç³»ç»Ÿ
ä½¿ç”¨AIæ·±åº¦ç†è§£æŠ€æœ¯å¯¹é€å­—ç¨¿è¿›è¡Œæ™ºèƒ½ä¼˜åŒ–
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from dataclasses import dataclass
from enum import Enum

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    DEFINITION = "definition"       # æ¦‚å¿µå®šä¹‰
    PROBLEM = "problem"             # é—®é¢˜åˆ†æ
    SOLUTION = "solution"           # è§£å†³æ–¹æ¡ˆ
    EXAMPLE = "example"             # ç¤ºä¾‹è¯´æ˜
    COMPARISON = "comparison"       # å¯¹æ¯”åˆ†æ
    PRINCIPLE = "principle"         # åŸç†è§£é‡Š
    IMPLEMENTATION = "implementation"  # å®ç°æ­¥éª¤
    SUMMARY = "summary"             # æ€»ç»“å½’çº³


@dataclass
class ContentBlock:
    """å†…å®¹å—æ•°æ®ç»“æ„"""
    type: ContentType
    title: str
    content: str
    subtopics: List[str] = None

    def __post_init__(self):
        if self.subtopics is None:
            self.subtopics = []


class IntelligentDocumentOptimizer:
    """æ™ºèƒ½æ–‡æ¡£ä¼˜åŒ–å™¨"""

    def __init__(self):
        # å£è¯­åŒ–è¯æ±‡åº“
        self.oral_patterns = {
            # å£æ’­è¯
            r'å¤§å®¶å¥½[ï¼Œã€‚]?': '',
            r'ä»Šå¤©(ç»™å¤§å®¶|æˆ‘è¦|æˆ‘ä»¬è¦)': '',
            r'åˆ«å¿˜äº†ç‚¹èµå…³æ³¨': '',
            r'æ¬¢è¿æ¥åˆ°.*?é¢‘é“': '',
            r'æˆ‘ä»¬ä¸‹æ¬¡å†è§': '',
            r'æ‹œæ‹œ': '',

            # å¡«å……è¯
            r'é‚£ä¹ˆ': '',
            r'å…¶å®å§?': '',
            r'åŸºæœ¬ä¸Š': '',
            r'å¯ä»¥è¯´': '',
            r'è¿™ä¸ªå‘¢': '',
            r'é‚£ä¸ªå‘¢': '',
            r'å‘¢[ï¼Œã€‚]': 'ã€‚',

            # å†—ä½™è¡¨è¾¾
            r'è¿™ä¸ªå°±æ˜¯(.+?)äº†': r'\1',
            r'(.+?)çš„è¯': r'\1',
            r'æˆ‘ä»¬ä¸å¦¨': '',
            r'è®©æˆ‘ä»¬': '',
            r'æ¥ä¸‹æ¥': '',

            # äº’åŠ¨è¯­å¥
            r'ä½ å¯èƒ½ä¼šé—®': '',
            r'ç›¸ä¿¡å¤§å®¶': '',
            r'ä¸çŸ¥é“ä½ æ˜¯å¦': '',
            r'å¤§å®¶å¯èƒ½': '',
            r'æœ‰æ²¡æœ‰æƒ³è¿‡': '',

            # æ—¶é—´æŒ‡ä»£
            r'åˆšæ‰(æˆ‘ä»¬)?': '',
            r'å‰é¢(æˆ‘ä»¬)?': '',
            r'åé¢(æˆ‘ä»¬)?ä¼š': '',
            r'ä¹‹å‰(æåˆ°|è¯´è¿‡)': '',
        }

        # ç¹ç®€è½¬æ¢å­—å…¸
        self.traditional_to_simplified = {
            'è¦–é »': 'è§†é¢‘', 'å•é¡Œ': 'é—®é¢˜', 'å¯¦éš›': 'å®é™…', 'é€™å€‹': 'è¿™ä¸ª',
            'é‹è¡Œ': 'è¿è¡Œ', 'åŸ·è¡Œ': 'æ‰§è¡Œ', 'è™•ç†': 'å¤„ç†', 'å¯¦ç¾': 'å®ç°',
            'æ‡‰ç”¨': 'åº”ç”¨', 'å„ªåŒ–': 'ä¼˜åŒ–', 'çµæ§‹': 'ç»“æ„', 'é‚è¼¯': 'é€»è¾‘',
            'æ•¸æ“š': 'æ•°æ®', 'ç¶²çµ¡': 'ç½‘ç»œ', 'è«‹æ±‚': 'è¯·æ±‚', 'éŸ¿æ‡‰': 'å“åº”',
            'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹ç™¼': 'å¼€å‘', 'æ¸¬è©¦': 'æµ‹è¯•', 'èª¿è©¦': 'è°ƒè¯•',
            'é …ç›®': 'é¡¹ç›®', 'ç”¢å“': 'äº§å“', 'ç’°å¢ƒ': 'ç¯å¢ƒ', 'è®Šé‡': 'å˜é‡',
            'é¡å‹': 'ç±»å‹', 'åƒæ•¸': 'å‚æ•°', 'è¿”å›': 'è¿”å›', 'èª¿ç”¨': 'è°ƒç”¨',
            'å‰µå»º': 'åˆ›å»º', 'åˆªé™¤': 'åˆ é™¤', 'ä¿®æ”¹': 'ä¿®æ”¹', 'æŸ¥è©¢': 'æŸ¥è¯¢',
            'å°å…¥': 'å¯¼å…¥', 'å°å‡º': 'å¯¼å‡º', 'å‚™ä»½': 'å¤‡ä»½', 'æ¢å¾©': 'æ¢å¤',
            'å•Ÿå‹•': 'å¯åŠ¨', 'é—œé–‰': 'å…³é—­', 'é‡å•Ÿ': 'é‡å¯', 'æš«åœ': 'æš‚åœ',
            'ç¹¼çºŒ': 'ç»§ç»­', 'å–æ¶ˆ': 'å–æ¶ˆ', 'ç¢ºèª': 'ç¡®è®¤', 'é¸æ“‡': 'é€‰æ‹©',
            'è¨­ç½®': 'è®¾ç½®', 'é…ç½®': 'é…ç½®', 'ç®¡ç†': 'ç®¡ç†', 'ç›£æ§': 'ç›‘æ§',
            'åˆ†æ': 'åˆ†æ', 'çµ±è¨ˆ': 'ç»Ÿè®¡', 'å ±å‘Š': 'æŠ¥å‘Š', 'æ—¥èªŒ': 'æ—¥å¿—',
            'éŒ¯èª¤': 'é”™è¯¯', 'è­¦å‘Š': 'è­¦å‘Š', 'ä¿¡æ¯': 'ä¿¡æ¯', 'æˆåŠŸ': 'æˆåŠŸ',
            'å¤±æ•—': 'å¤±è´¥', 'ç•°å¸¸': 'å¼‚å¸¸', 'æ­£å¸¸': 'æ­£å¸¸', 'ç‹€æ…‹': 'çŠ¶æ€',
            'é€²åº¦': 'è¿›åº¦', 'å®Œæˆ': 'å®Œæˆ', 'å¾…è¾¦': 'å¾…åŠ', 'é€²è¡Œä¸­': 'è¿›è¡Œä¸­',
            'æ­·å²': 'å†å²', 'è¨˜éŒ„': 'è®°å½•', 'ç·©å­˜': 'ç¼“å­˜', 'å…§å­˜': 'å†…å­˜',
            'ç¡¬ç›¤': 'ç¡¬ç›˜', 'æ–‡ä»¶': 'æ–‡ä»¶', 'ç›®éŒ„': 'ç›®å½•', 'è·¯å¾‘': 'è·¯å¾„',
            'éˆæ¥': 'é“¾æ¥', 'åœ°å€': 'åœ°å€', 'ç«¯å£': 'ç«¯å£', 'å”è­°': 'åè®®',
            'åŠ å¯†': 'åŠ å¯†', 'è§£å¯†': 'è§£å¯†', 'èªè­‰': 'è®¤è¯', 'æˆæ¬Š': 'æˆæƒ',
            'æ¬Šé™': 'æƒé™', 'è§’è‰²': 'è§’è‰²', 'ç”¨æˆ¶': 'ç”¨æˆ·', 'è³¬è™Ÿ': 'è´¦å·',
            'å¯†ç¢¼': 'å¯†ç ', 'ç™»éŒ„': 'ç™»å½•', 'è¨»å†Š': 'æ³¨å†Œ', 'é€€å‡º': 'é€€å‡º',
            'æœƒè©±': 'ä¼šè¯', 'é€£æ¥': 'è¿æ¥', 'æ–·é–‹': 'æ–­å¼€', 'è¶…æ™‚': 'è¶…æ—¶',
            'ä¸¦ç™¼': 'å¹¶å‘', 'åŒæ­¥': 'åŒæ­¥', 'ç•°æ­¥': 'å¼‚æ­¥', 'éšŠåˆ—': 'é˜Ÿåˆ—',
            'ç·šç¨‹': 'çº¿ç¨‹', 'é€²ç¨‹': 'è¿›ç¨‹', 'æœå‹™': 'æœåŠ¡', 'å®¢æˆ¶ç«¯': 'å®¢æˆ·ç«¯',
            'æœå‹™å™¨': 'æœåŠ¡å™¨', 'æ•¸æ“šåº«': 'æ•°æ®åº“', 'è¡¨æ ¼': 'è¡¨æ ¼', 'å­—æ®µ': 'å­—æ®µ',
            'ç´¢å¼•': 'ç´¢å¼•', 'ä¸»éµ': 'ä¸»é”®', 'å¤–éµ': 'å¤–é”®', 'ç´„æŸ': 'çº¦æŸ',
            'è§¸ç™¼å™¨': 'è§¦å‘å™¨', 'å­˜å„²éç¨‹': 'å­˜å‚¨è¿‡ç¨‹', 'è¦–åœ–': 'è§†å›¾', 'å‡½æ•¸': 'å‡½æ•°'
        }

        # æŠ€æœ¯æœ¯è¯­ä¿®æ­£
        self.term_corrections = {
            'æ–¹å…‹æ£®é 0': 'Function Calling',
            'æ–¹å…‹æ£®Calling': 'Function Calling',
            'æè³ªBT': 'ChatGPT',
            'ChaBT': 'ChatGPT',
            'å®‰æ–¯ç¾…åŸ¹å…‹': 'Anthropic',
            'å¤§æ‘©æ˜Ÿ': 'å¤§æ¨¡å‹',
            'GiveBarkest': 'GetWeather',
            'Closed AI': 'OpenAI',
            'ClawD': 'Claude',
            'Gemnet': 'Gemini',
            'ä¸Šä¸‹è¼ªçª—å£': 'Context Window',
            'ä¸Šä¸‹è½®çª—å£': 'Context Window',
            'Context and Engineering': 'Context Engineering',
            'MALSEAZEN': 'Multi-Agent',
            'MaltzAgent': 'Multi-Agent',
            'Mouth Agents': 'Multi-Agent',
        }

    def convert_traditional_to_simplified(self, text: str) -> str:
        """ç¹ä½“è½¬ç®€ä½“"""
        for trad, simp in self.traditional_to_simplified.items():
            text = text.replace(trad, simp)
        return text

    def fix_technical_terms(self, text: str) -> str:
        """ä¿®æ­£æŠ€æœ¯æœ¯è¯­"""
        for wrong, correct in self.term_corrections.items():
            text = re.sub(re.escape(wrong), correct, text, flags=re.IGNORECASE)
        return text

    def remove_oral_expressions(self, text: str) -> str:
        """å»é™¤å£è¯­åŒ–è¡¨è¾¾"""
        for pattern, replacement in self.oral_patterns.items():
            text = re.sub(pattern, replacement, text)

        # å»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r' ([ï¼Œã€‚ï¼ï¼Ÿï¼›])', r'\1', text)

        return text.strip()

    def identify_content_type(self, text: str) -> ContentType:
        """è¯†åˆ«å†…å®¹ç±»å‹"""
        text_lower = text.lower()

        # å®šä¹‰ç±»å‹è¯†åˆ«è§„åˆ™
        patterns = {
            ContentType.DEFINITION: [
                r'æ˜¯ä»€ä¹ˆ', r'ä»€ä¹ˆæ˜¯', r'å®šä¹‰', r'æ¦‚å¿µ', r'æŒ‡çš„æ˜¯', r'å«ä¹‰',
                r'æ˜¯ä¸€ç§', r'æ˜¯ä¸€ä¸ª', r'ç®€å•æ¥è¯´'
            ],
            ContentType.PROBLEM: [
                r'é—®é¢˜', r'å›°éš¾', r'æŒ‘æˆ˜', r'é™åˆ¶', r'ç“¶é¢ˆ', r'ç¼ºé™·',
                r'ä¸ºä»€ä¹ˆéœ€è¦', r'ä¸ºä»€ä¹ˆè¦', r'è§£å†³äº†ä»€ä¹ˆ'
            ],
            ContentType.SOLUTION: [
                r'è§£å†³æ–¹æ¡ˆ', r'æ–¹æ³•', r'æŠ€æœ¯', r'å®ç°', r'ç­–ç•¥',
                r'å¦‚ä½•', r'æ€ä¹ˆ', r'æ­¥éª¤', r'æµç¨‹'
            ],
            ContentType.EXAMPLE: [
                r'ä¾‹å­', r'ç¤ºä¾‹', r'æ¡ˆä¾‹', r'ä¸¾ä¾‹', r'æ¯”å¦‚', r'è­¬å¦‚'
            ],
            ContentType.COMPARISON: [
                r'å¯¹æ¯”', r'æ¯”è¾ƒ', r'åŒºåˆ«', r'ä¸åŒ', r'ç›¸åŒ', r'ç±»ä¼¼',
                r'ä¸.*ç›¸æ¯”', r'è€Œ.*åˆ™'
            ],
            ContentType.PRINCIPLE: [
                r'åŸç†', r'æœºåˆ¶', r'å·¥ä½œæ–¹å¼', r'åº•å±‚', r'æœ¬è´¨'
            ],
            ContentType.IMPLEMENTATION: [
                r'å®æ–½', r'éƒ¨ç½²', r'é…ç½®', r'å®‰è£…', r'æ“ä½œ'
            ],
            ContentType.SUMMARY: [
                r'æ€»ç»“', r'æ€»ä¹‹', r'ç»¼ä¸Š', r'å›é¡¾', r'å°ç»“'
            ]
        }

        # ç»Ÿè®¡å„ç±»å‹çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for content_type, pattern_list in patterns.items():
            score = sum(1 for pattern in pattern_list if re.search(pattern, text))
            scores[content_type] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        return ContentType.DEFINITION  # é»˜è®¤ä¸ºå®šä¹‰ç±»å‹

    def extract_key_points(self, text: str) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        key_points = []

        # å…³é”®æŒ‡ç¤ºè¯
        key_indicators = [
            'æ ¸å¿ƒ', 'å…³é”®', 'é‡è¦', 'ä¸»è¦', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å',
            'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'åŒ…æ‹¬', 'åˆ†ä¸º', 'ç”±äº', 'å› æ­¤'
        ]

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # è¿‡çŸ­çš„å¥å­è·³è¿‡
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æŒ‡ç¤ºè¯
            if any(indicator in sentence for indicator in key_indicators):
                key_points.append(sentence)
            # æˆ–è€…åŒ…å«æŠ€æœ¯æœ¯è¯­
            elif any(term in sentence for term in ['API', 'Context', 'Token', 'Agent', 'Model']):
                key_points.append(sentence)

        return key_points[:10]  # æœ€å¤šè¿”å›10ä¸ªè¦ç‚¹

    def organize_content_blocks(self, text: str) -> List[ContentBlock]:
        """ç»„ç»‡å†…å®¹å—"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\n+', text)
        content_blocks = []

        for para in paragraphs:
            if len(para.strip()) < 50:  # è¿‡çŸ­çš„æ®µè½è·³è¿‡
                continue

            # è¯†åˆ«å†…å®¹ç±»å‹
            content_type = self.identify_content_type(para)

            # æå–æ ‡é¢˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            title_match = re.match(r'^#+\s*(.+?)(?:\n|$)', para)
            if title_match:
                title = title_match.group(1)
                content = para[title_match.end():].strip()
            else:
                # æ ¹æ®å†…å®¹ç±»å‹ç”Ÿæˆé»˜è®¤æ ‡é¢˜
                title = self._generate_title(para, content_type)
                content = para

            # æå–å­ä¸»é¢˜
            subtopics = self.extract_key_points(content)[:3]

            block = ContentBlock(
                type=content_type,
                title=title,
                content=content,
                subtopics=subtopics
            )
            content_blocks.append(block)

        return content_blocks

    def _generate_title(self, text: str, content_type: ContentType) -> str:
        """ç”Ÿæˆæ ‡é¢˜"""
        # æå–å‰20ä¸ªå­—ä½œä¸ºåŸºç¡€
        preview = text[:50].split('ã€‚')[0]

        # æ ¹æ®ç±»å‹æ·»åŠ å‰ç¼€
        type_prefixes = {
            ContentType.DEFINITION: "æ¦‚å¿µï¼š",
            ContentType.PROBLEM: "é—®é¢˜ï¼š",
            ContentType.SOLUTION: "æ–¹æ¡ˆï¼š",
            ContentType.EXAMPLE: "ç¤ºä¾‹ï¼š",
            ContentType.COMPARISON: "å¯¹æ¯”ï¼š",
            ContentType.PRINCIPLE: "åŸç†ï¼š",
            ContentType.IMPLEMENTATION: "å®ç°ï¼š",
            ContentType.SUMMARY: "æ€»ç»“ï¼š"
        }

        prefix = type_prefixes.get(content_type, "")

        # å°è¯•æå–å…³é”®æ¦‚å¿µ
        tech_terms = re.findall(r'[A-Z][a-zA-Z]+\s*[A-Z]*[a-zA-Z]*', preview)
        if tech_terms:
            return prefix + tech_terms[0]

        # ä½¿ç”¨å‰å‡ ä¸ªè¯
        words = preview.split()[:5]
        return prefix + ' '.join(words)

    def generate_structured_document(self, blocks: List[ContentBlock]) -> str:
        """ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£"""
        doc = []

        # æŒ‰å†…å®¹ç±»å‹åˆ†ç»„
        grouped = {}
        for block in blocks:
            if block.type not in grouped:
                grouped[block.type] = []
            grouped[block.type].append(block)

        # å®šä¹‰è¾“å‡ºé¡ºåº
        output_order = [
            ContentType.DEFINITION,
            ContentType.PROBLEM,
            ContentType.PRINCIPLE,
            ContentType.SOLUTION,
            ContentType.IMPLEMENTATION,
            ContentType.EXAMPLE,
            ContentType.COMPARISON,
            ContentType.SUMMARY
        ]

        # æŒ‰é¡ºåºè¾“å‡º
        for content_type in output_order:
            if content_type not in grouped:
                continue

            # æ·»åŠ å¤§ç« èŠ‚æ ‡é¢˜
            section_titles = {
                ContentType.DEFINITION: "## æ ¸å¿ƒæ¦‚å¿µ",
                ContentType.PROBLEM: "## é—®é¢˜åˆ†æ",
                ContentType.PRINCIPLE: "## æŠ€æœ¯åŸç†",
                ContentType.SOLUTION: "## è§£å†³æ–¹æ¡ˆ",
                ContentType.IMPLEMENTATION: "## å®ç°æ­¥éª¤",
                ContentType.EXAMPLE: "## åº”ç”¨ç¤ºä¾‹",
                ContentType.COMPARISON: "## å¯¹æ¯”åˆ†æ",
                ContentType.SUMMARY: "## æ€»ç»“"
            }

            doc.append(section_titles[content_type])
            doc.append("")

            # è¾“å‡ºè¯¥ç±»å‹çš„æ‰€æœ‰å†…å®¹å—
            for block in grouped[content_type]:
                # å­æ ‡é¢˜
                doc.append(f"### {block.title}")
                doc.append("")

                # å†…å®¹
                # ä¼˜åŒ–å†…å®¹æ ¼å¼
                optimized_content = self._optimize_block_content(block.content)
                doc.append(optimized_content)
                doc.append("")

                # å¦‚æœæœ‰å­è¦ç‚¹ï¼Œä»¥åˆ—è¡¨å½¢å¼å±•ç¤º
                if block.subtopics:
                    doc.append("**è¦ç‚¹ï¼š**")
                    for point in block.subtopics:
                        doc.append(f"- {point}")
                    doc.append("")

        return '\n'.join(doc)

    def _optimize_block_content(self, content: str) -> str:
        """ä¼˜åŒ–å—å†…å®¹"""
        # åˆ†å¥å¤„ç†
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', content)
        optimized = []

        current_para = []
        for i in range(0, len(sentences)-1, 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')
            sentence = sentence.strip()

            if not sentence:
                continue

            current_para.append(sentence)

            # æ¯3-5å¥åˆ†æ®µ
            if len(current_para) >= 3 and sentence.endswith('ã€‚'):
                optimized.append(''.join(current_para))
                current_para = []

        if current_para:
            optimized.append(''.join(current_para))

        return '\n\n'.join(optimized)

    def optimize_document(self, text: str) -> str:
        """ä¼˜åŒ–æ–‡æ¡£ä¸»æµç¨‹"""
        logger.info("å¼€å§‹æ™ºèƒ½æ–‡æ¡£ä¼˜åŒ–...")

        # Step 1: ç¹ç®€è½¬æ¢
        logger.info("Step 1: ç¹ç®€ä½“è½¬æ¢")
        text = self.convert_traditional_to_simplified(text)

        # Step 2: æŠ€æœ¯æœ¯è¯­ä¿®æ­£
        logger.info("Step 2: æŠ€æœ¯æœ¯è¯­ä¿®æ­£")
        text = self.fix_technical_terms(text)

        # Step 3: å»é™¤å£è¯­åŒ–è¡¨è¾¾
        logger.info("Step 3: æ¸…ç†å£è¯­åŒ–å†…å®¹")
        text = self.remove_oral_expressions(text)

        # Step 4: è¯†åˆ«å’Œç»„ç»‡å†…å®¹å—
        logger.info("Step 4: å†…å®¹ç»“æ„åˆ†æ")
        blocks = self.organize_content_blocks(text)

        # Step 5: ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£
        logger.info("Step 5: ç”Ÿæˆä¼˜åŒ–æ–‡æ¡£")
        structured_doc = self.generate_structured_document(blocks)

        return structured_doc


def optimize_file(input_path: str, output_path: str) -> bool:
    """ä¼˜åŒ–å•ä¸ªæ–‡ä»¶"""
    try:
        optimizer = IntelligentDocumentOptimizer()

        # è¯»å–åŸå§‹æ–‡ä»¶
        with open(input_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–æ ‡é¢˜
        lines = content.split('\n')
        title = lines[0].replace('#', '').strip() if lines else Path(input_path).stem

        # ä¼˜åŒ–å†…å®¹
        optimized_content = optimizer.optimize_document('\n'.join(lines[1:]))

        # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
        final_doc = f"# {title}\n\n## æ¦‚è¿°\n\næœ¬æ–‡æ¡£åŸºäºæŠ€æœ¯è§†é¢‘é€å­—ç¨¿æ·±åº¦ä¼˜åŒ–ç”Ÿæˆï¼Œå»é™¤äº†å£è¯­åŒ–å†…å®¹ï¼Œé‡æ„äº†é€»è¾‘ç»“æ„ã€‚\n\n{optimized_content}"

        # ä¿å­˜ä¼˜åŒ–åçš„æ–‡æ¡£
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(final_doc)

        logger.info(f"âœ… ä¼˜åŒ–å®Œæˆ: {Path(output_path).name}")
        return True

    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ–‡æ¡£æ·±åº¦ä¼˜åŒ–')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•')

    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)

    if input_path.is_file():
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        optimize_file(str(input_path), str(output_path))
    elif input_path.is_dir():
        # æ‰¹é‡å¤„ç†
        md_files = list(input_path.glob("*.md"))
        success = 0

        for md_file in md_files:
            out_file = output_path / f"intelligent_{md_file.name}"
            if optimize_file(str(md_file), str(out_file)):
                success += 1

        logger.info(f"\nğŸ“Š æ‰¹é‡ä¼˜åŒ–å®Œæˆ: {success}/{len(md_files)} æˆåŠŸ")
    else:
        logger.error(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")


if __name__ == "__main__":
    main()